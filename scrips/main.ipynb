{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "readdone\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import process_json as pj\n",
    "\n",
    "#must use relative path\n",
    "#ipython's default path is different from python's\n",
    "filename_night1 = \"../201206_tweets/activities_201206100000_201206100010.json\"\n",
    "filename_night2 = \"../201206_tweets/activities_201206100010_201206100020.json\"\n",
    "filename_night3 = \"../201206_tweets/activities_201206100020_201206100030.json\"\n",
    "filename_night4 = \"../201206_tweets/activities_201206100030_201206100040.json\"\n",
    "filename_night5 = \"../201206_tweets/activities_201206100040_201206100050.json\"\n",
    "filename_night6 = \"../201206_tweets/activities_201206100050_201206100100.json\"\n",
    "filename_night7 = \"../201206_tweets/activities_201206100100_201206100110.json\"\n",
    "filename_night8 = \"../201206_tweets/activities_201206100110_201206100120.json\"\n",
    "filename_night9 = \"../201206_tweets/activities_201206100120_201206100130.json\"\n",
    "filename_night10 = \"../201206_tweets/activities_201206100130_201206100140.json\"\n",
    "filename_night11 = \"../201206_tweets/activities_201206100140_201206100150.json\"\n",
    "filename_night12 = \"../201206_tweets/activities_201206100150_201206102100.json\"\n",
    "\n",
    "filename_day1 = \"../201206_tweets/activities_201206101200_201206101210.json\"\n",
    "filename_day2 = \"../201206_tweets/activities_201206101210_201206101220.json\"\n",
    "filename_day3 = \"../201206_tweets/activities_201206101220_201206101230.json\"\n",
    "filename_day4 = \"../201206_tweets/activities_201206101230_201206101240.json\"\n",
    "filename_day5 = \"../201206_tweets/activities_201206101240_201206101250.json\"\n",
    "filename_day6 = \"../201206_tweets/activities_201206101250_201206101300.json\"\n",
    "filename_day7 = \"../201206_tweets/activities_201206101300_201206101310.json\"\n",
    "filename_day8 = \"../201206_tweets/activities_201206101310_201206101320.json\"\n",
    "filename_day9 = \"../201206_tweets/activities_201206101320_201206101330.json\"\n",
    "filename_day10 = \"../201206_tweets/activities_201206101330_201206101340.json\"\n",
    "filename_day11 = \"../201206_tweets/activities_201206101340_201206101350.json\"\n",
    "filename_day12 = \"../201206_tweets/activities_201206101350_201206101400.json\"\n",
    "json_processor = pj.process_json()\n",
    "papers_night = []\n",
    "papers_night = json_processor.read_single_file(filename_night1,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night2,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night3,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night4,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night5,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night6,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night7,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night8,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night9,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night10,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night11,papers_night)\n",
    "papers_night = json_processor.read_single_file(filename_night12,papers_night)\n",
    "json_processor.draw_geo(papers_night)\n",
    "papers_day = []\n",
    "papers_day = json_processor.read_single_file(filename_day1,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day2,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day3,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day4,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day5,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day6,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day7,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day8,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day9,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day10,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day11,papers_day)\n",
    "papers_day = json_processor.read_single_file(filename_day12,papers_day)\n",
    "json_processor.draw_geo(papers_day)\n",
    "\n",
    "papers = papers_day+papers_night\n",
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2)\n",
      "(13, 2)\n"
     ]
    }
   ],
   "source": [
    "#creat dataset\n",
    "from sklearn.preprocessing import minmax_scale,StandardScaler\n",
    "import numpy as np\n",
    "geos_day,geos_night,day_label,night_label = [],[],[],[]\n",
    "for line in papers_day:\n",
    "    geos_day.append(line[\"geo\"][\"coordinates\"])\n",
    "    day_label = day_label+[1]\n",
    "for line in papers_night:\n",
    "    geos_night.append(line[\"geo\"][\"coordinates\"])\n",
    "    night_label = night_label+[0]\n",
    "    \n",
    "geos_day = np.array(geos_day)\n",
    "geos_night = np.array(geos_night)\n",
    "day_label,night_label = np.row_stack(day_label),np.row_stack(night_label)\n",
    "print(geos_day.shape)\n",
    "print((geos_night.shape))\n",
    "\n",
    "percent = 0.8\n",
    "partition = int(geos_day.shape[0]*percent)\n",
    "train_x_d, train_y_d, val_x_d, val_y_d = geos_day[:partition],day_label[:partition], geos_day[partition:], day_label[partition:]\n",
    "\n",
    "percent = 0.8\n",
    "partition = int(geos_night.shape[0]*percent)\n",
    "train_x_n, train_y_n, val_x_n, val_y_n = geos_night[:partition],night_label[:partition], geos_night[partition:], night_label[partition:]\n",
    "\n",
    "train_x = np.vstack((train_x_d,train_x_n))\n",
    "train_y = np.vstack((train_y_d,train_y_n))\n",
    "val_x = np.vstack((val_x_d,val_x_n))\n",
    "val_y = np.vstack((val_y_d,val_y_n))\n",
    "\n",
    "#归一化\n",
    "#target must 0D or 1D\n",
    "train_y = train_y.squeeze()\n",
    "val_y = val_y.squeeze()\n",
    "train_x = minmax_scale(train_x)\n",
    "val_x = minmax_scale(val_x)\n",
    "# print(train_x.shape,train_y.shape,val_x.shape,val_y.shape)\n",
    "# print(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dataset\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeoDataset(Dataset):\n",
    "    #有可能一次处理一个trains[i]\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.tensor(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.label=torch.tensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset and dataloader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = GeoDataset(train_x, train_y)\n",
    "val_set = GeoDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (fc3): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#用 全连接神经网络 进行分辨\n",
    "#目标：输出三色，白天和黑夜重合的部分输出绿色\n",
    "# 首先要引入相关的包\n",
    "import torch\n",
    "# 引入torch.nn并指定别名\n",
    "import torch.nn as nn\n",
    "#打印一下版本\n",
    "torch.__version__\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module子类的函数必须在构造函数中执行父类的构造函数\n",
    "        super(Classifier, self).__init__()\n",
    "        #线性层，输入1350个特征，输出10个特征\n",
    "        self.fc1   = nn.Linear(2, 5)  #这里的1350是如何计算的呢？这就要看后面的forward函数\n",
    "        self.fc2   = nn.Linear(5, 5)  #这里的1350是如何计算的呢？这就要看后面的forward函数\n",
    "        self.fc3   = nn.Linear(5, 2)  #这里的1350是如何计算的呢？这就要看后面的forward函数\n",
    "    #正向传播 \n",
    "    def forward(self, x): \n",
    "        # print(x.size()) # 结果：[1, 1, 32, 32]\n",
    "        # 卷积 -> 激活 -> 池化 \n",
    "        x = self.fc1(x) \n",
    "        x = F.leaky_relu(x)\n",
    "        # print(x.size()) # 结果：[1, 6, 30, 30]\n",
    "        x = self.fc2(x) \n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # print(x.size()) # 结果：[1, 6, 15, 15]\n",
    "        # # reshape，‘-1’表示自适应\n",
    "        # #这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]\n",
    "        # x = x.view(x.size()[0], -1) \n",
    "        # print(x.size()) # 这里就是fc1层的的输入1350 \n",
    "        # x = self.fc1(x)        \n",
    "        return x\n",
    "\n",
    "net = Classifier()\n",
    "print(net)\n",
    "\n",
    "#check device\n",
    "def get_device():\n",
    "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the training parameters here.\n",
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = get_device()\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 20              # number of training epoch\n",
    "learning_rate = 0.1      # learning rate\n",
    "\n",
    "# the path where checkpoint saved\n",
    "model_path = './model.ckpt'\n",
    "\n",
    "# create model, define a loss function, and optimizer\n",
    "model = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[001/020] Train Acc: 0.677419 Loss: 0.672619 | Val Acc: 0.666667 loss: 0.663968\n",
      "saving model with acc 0.667\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[002/020] Train Acc: 0.677419 Loss: 0.655657 | Val Acc: 0.666667 loss: 0.650319\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[003/020] Train Acc: 0.677419 Loss: 0.643986 | Val Acc: 0.666667 loss: 0.640476\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[004/020] Train Acc: 0.677419 Loss: 0.641215 | Val Acc: 0.666667 loss: 0.636544\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[005/020] Train Acc: 0.677419 Loss: 0.632886 | Val Acc: 0.666667 loss: 0.635902\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[006/020] Train Acc: 0.677419 Loss: 0.626334 | Val Acc: 0.666667 loss: 0.636270\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[007/020] Train Acc: 0.677419 Loss: 0.628571 | Val Acc: 0.666667 loss: 0.637103\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[008/020] Train Acc: 0.677419 Loss: 0.626707 | Val Acc: 0.666667 loss: 0.637600\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[009/020] Train Acc: 0.677419 Loss: 0.631103 | Val Acc: 0.666667 loss: 0.638092\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[010/020] Train Acc: 0.677419 Loss: 0.630956 | Val Acc: 0.666667 loss: 0.638248\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[011/020] Train Acc: 0.677419 Loss: 0.630634 | Val Acc: 0.666667 loss: 0.638077\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[012/020] Train Acc: 0.677419 Loss: 0.632013 | Val Acc: 0.666667 loss: 0.637907\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[013/020] Train Acc: 0.677419 Loss: 0.626052 | Val Acc: 0.666667 loss: 0.637038\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[014/020] Train Acc: 0.677419 Loss: 0.630269 | Val Acc: 0.666667 loss: 0.636567\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[015/020] Train Acc: 0.677419 Loss: 0.625026 | Val Acc: 0.666667 loss: 0.635432\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[016/020] Train Acc: 0.677419 Loss: 0.624642 | Val Acc: 0.666667 loss: 0.641052\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[017/020] Train Acc: 0.677419 Loss: 0.618584 | Val Acc: 0.555556 loss: 0.726364\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[018/020] Train Acc: 0.677419 Loss: 0.610635 | Val Acc: 0.555556 loss: 0.731405\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
      "[019/020] Train Acc: 0.709677 Loss: 0.577501 | Val Acc: 0.555556 loss: 0.728770\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "[020/020] Train Acc: 0.741935 Loss: 0.577557 | Val Acc: 0.555556 loss: 0.728454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lanpokn/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "#问题：交叉熵不要自己独热化，给0到n-1作为标签即可\n",
    "# 为了解决：expected scalar type Float but found Double\n",
    "model = model.double()\n",
    "#可能是因为没有初始化？\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "model.apply(init_weights)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(inputs),print(labels)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        # print(f'Train:{outputs}')\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        # print(f'train:{train_pred}')\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        print(train_pred.cpu())\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
